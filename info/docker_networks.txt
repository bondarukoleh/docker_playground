When we start the container and make a -p (expose ports --publish) on it -> we connecting a particular (конкретной) private network. 
So it's a virtual private network "bridge" (by default)
Each of these networks routs through NAT firewall on host machine IP, it's a special containers demon (service) with witch
containers can access outer network and your own
* All the containers can talk to each other inside our host, even without -p. So for example we have mysql and php apache 
containers, and they can talk to each other without setting a bounded port.(-p)
* Good practice is to create a new virtual network for each logical connected containers.
 network "my_web_app" for mysql and php, or "my_api" for mongo and nodejs containers.
* Many of network setting can be configurable.
So we can:
- create a private network per app.
- Attach containers to more then 1 virtual network (or none).
- Skip virtual network and use host IP (--new=host)

Docker network drivers could be changed to get new abilities.

To get settings of container - we can
$>docker container inspect container_name | grep "needed setting" 
But it has such flag as --format
$>docker container inspect container_name --format '{{ .NeededSetting.NeededSubSetting }}'
$>docker container inspect container_name --format '{{ .NetworkSettings.IPAddress }}' -> 172.17.0.2

And address of my machine is 
$>ifconfig en0 -> inet 172.29.220.21

As we can see they are not the same.
So the container network explanation:
When we start a container, and give it -p 3000:2000 that means bind 3000 port on host machine and redirect all traffic 
from it to 2000 port in container. Redirection done via virtual network. By default it is bridge, you can check it
via "inspect" command (in NetworkSettings).
"NetworkSettings": {
 "Ports": {
                "80/tcp": [
                    {
                        "HostIp": "0.0.0.0",
                        "HostPort": "3000"
                    }
                ]
            }
 "Networks": {
                "bridge": {
                    "Gateway": "172.17.0.1",
                    "IPAddress": "172.17.0.2",
                    "IPPrefixLen": 16,
                    "MacAddress": "02:42:ac:11:00:02",
                    ...
                }
            }
...
}

And containers that are in this bridge virtual network can talk to each other. That's why we can put a few containers in
some other virtual network ("my_app" with nginx and node containers), and they will be in separate virtual network.

So when we open bind a port to container - we break thru host firewall and make this binding thru virtual network.

-------------\     ---------         ---------   ------------  
--------------\   | Host    |       | Virtual | |            |   
Incoming Traffic  | Firewall| ----> | "bridge"| | Container1 | 
--------------/   |         |       | Network | |            |
-------------/     ---------  \       ---------  ------------
                               \                              
                                \    ---------   ------------  
                                 \  | Virtual | |            |   
                                    | "my_app"| | Container2 | 
                                    | Network | |            |
                                     ---------   ------------
  
To make these two containers talk to each other they have to go thru public port of host machine.
And you do remember that we can bind public port only once. So if container1 is -p 8080:4000 
we cannot make container2 is -p 8080:4444, because it's already taken. Also as you get it container2 can speak to 
container1 via hostIp:8080.

CLI commands:
$>docker network ls -> show networks.
NETWORK ID          NAME                DRIVER              SCOPE
fb4e3697e5e6        bridge              bridge              local
69a93a2e2232        host                host                local
c44933144555        none                null                local

Bridge - is a default virtual network, that bridges traffic from your container thru NET Firewall up
  to physical network that host is connected to.
Host - if we put containers in it, this network can bind container directly PN (physical network) of 
  the host, witch can gain performance of network traffic by skipping VN (Virtual Network) proxy, but
  sacrifices security of container.
None - removes eth0 and leaves you with 


$>docker network inspect network_name -> (bridge, host, etc) returns a json array. as you can see -
 we can inspect everything.
 ...
 "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16", -> default subnet for all containers 
                    "Gateway": "172.17.0.1" -> gateway to go to physical hos network.
                }
            ]
        },
 ...
 "Containers": {
            "9fa59e0bdd4e13a99231699cc5e4de9dc9aa9842108242433402478a6a712dc0": {
                "Name": "ng",
                "EndpointID": "8a9716e4fcac5438f7dd6f572be6b740296e8dc5b9b734c2d5937c2b76b8e2e9",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            }
        } -> There are running containers in this virtual network, and we can check them.
 ...

$>docker network create --driver -> crate a network
$>Attach network connect -> Attach a network to container
$>Detach network connect -> Container network disconnect a network to container
  