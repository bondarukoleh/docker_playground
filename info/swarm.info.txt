Swarm is compose of nodes.
Node - is a base unit of swarm, simple machine running docker.
Two types of nodes - Manager nodes, that responsible for managing nodes, 
orchestration them and other and Worker nodes - that must only do some work and report 
about the result.
  Manager nodes are collected in manager quorum, where they speaking to each other with
RAFT protocol, more consistent and straight.
  Worker nodes collected in worker quorum where they speaking wih gossip protocol less
consistent and bulk.
  Managers talk with workers via GRPS protocol - fast protocol built over http2.
  Roles depend on your realization and can be changed dynamically.

Managers have their leader, else managers - are followers, they all forward their traffic
to their leader. Leader is responsible for orchestration of processes, if leader
goes down - managers will chose another leader from them via RAFT protocol.

Managers quorum - has some internal data store, shared with every manager in quorum, so 
we don't need another one as a dependency. If we look in details, each manager has it's 
own datastore that is a copy of every manager datastore if everything is ok with connection.
Datastore state is managed by leader, and leader tells every follower which data needs to be
stored. Another advantage - is performance.

Every manager in swarm have Certificate Authority (which makes them managers), and it can 
generate identity secret for worker, so any node without this identity cannot be connected.

Workers aware of all managers, managers always tell workers here there are, and if a new
manger is in, and if one goes down - workers that were connected to internal
will reconnect to another active.

Orchestration.
You as a user want new service.
Manager node:
 - API - accepts commands from client and creates service object. Checks that everything
  is ok with your service, and store it to data store, ping orchestrator to create a service.
 - Orchestrator checks is there any of these services running already, if not - it creates
task in sheduler to create a service. Loop for service objects.
 - Allocator - allocates IP addresses to tasks.
 - Sheduler picks a task and checks is there is an available machine (node, worker) do do this 
task. And it ask this info from Dispatcher. Assigns tasks to nodes.
 - Dispatcher knows about every worker that tells him about himself, how busy he is, 
what he can do etc, so dispatcher pass the task to watcher and keeps looking how 
the things is going with it.

Worker node:
 - Worker - connects to dispatcher to check on assigned tasks.
 - Executor - executes a task.

If the worker is down, dispatcher understands that worker is no longer responding on ping, 
and it marks this worker as a down one. Orchestrator get this information and check is 
this worker is needed, if it is so - the flow is the same as with user, orchestration
will create a task and things will go as usual.

So swarm is a instrument that can help you to collect as many nodes, machines, dockers
as you want in one manageable unit.
Docker produces a swarmkit - it's a toolkit to create swarm.

So now we instead of run command will use service command. Each task - is a container that 
needs to be run.

So we can check if swarm is active be docker info. "Swarm inactive"
we can do 
$>docker swarm init -> and swarm did init
Swarm initialized: current node (xmzuljr8l0747z4lm53nvtu1q) is now a manager.
To add a worker to this swarm, run the following command:
    docker swarm join --token SWMTKN-1-4es3n96jx3pkjt9glkfgpbe5zcbjghcer2aoma4xyaotj0xb40-\
    147nh4rins1hs8sb2lu8kbjik your_ip_here
To add a manager to this swarm, run 'docker swarm join-token manager' and follow the
 instructions.

Under the hood some work has done:
Certificate created for swarm, special certificate created for current node. Join tokens are
created to join the swarm, database is created to store certificates, and all orchestration 
 information, etc.
 
$>docker node ls
ID                          HOSTNAME         STATUS  AVAILABILITY  MANAGER STATUS ENGINE VERSION
xmzuljr8l0747z4lm53nvtu1q * oleh-VirtualBox  Ready   Active        Leader         18.09.6

docker node command is to bringing your node in and out of the swarm, make node a manager or 
worker, etc.

docker swarm - is for initialize, join leave from swarm.

Creating a swarm - has different philosophy rather then docker container. If we working with
containers - we always think about it like a main sing, all spinning around a host-container
and so on. When creating a cluster (swarm) - containers loose their individuality, we working
with them as with tasks, and they don't need names and so on. we just throw a requirements
for the service and swarm is resolving it, that's  all.

$>docker service create alpine ping 8.8.8.8
n80hhik4vbgjj052s0hl1okxs -> service id, not container
overall progress: 1 out of 1 tasks 
1/1: running   
verify: Service converged

$>docker service ls -> to show a services
ID           NAME       MODE       REPLICAS IMAGE         PORTS
n80hhik4vbgj zen_morse  replicated 1/1      alpine:latest
REPLICAS - 1/1 how_many_services_running/how_many_you've_asked_to_run

$>docker service ps zen_morse -> to show the task.
ID           NAME        IMAGE         NODE            DESIRED STATE CURRENT STATE         ERROR PORTS
q410bnacjmkj zen_morse.1 alpine:latest oleh-VirtualBox Running       Running 3 minutes ago 

$>docker container ls -> it still works, because it still a container running.
CONTAINER ID IMAGE         COMMAND        CREATED       STATUS       PORTS 
d919dcfeb805 alpine:latest "ping 8.8.8.8" 5 minutes ago Up 5 minutes
NAMES
zen_morse.1.q410bnacjmkjnhfmpmg6xlzkr -> you can see that it has random name given by a swarm.

to change something - we can
$>docker service update n80hhik4vbgj/zen_morse (service id, or name) --replicas 3
$>docker service ls
ID           NAME        MODE          REPLICAS  IMAGE
n80hhik4vbgj zen_morse   replicated    3/3       alpine:latest 
$>docker service ps zen_morse
ID           NAME        IMAGE         NODE            DESIRED STATE CURRENT STATE          
q410bnacjmkj zen_morse.1 alpine:latest oleh-VirtualBox Running       Running 10 minutes ago                       
znd66x7bs4jd zen_morse.2 alpine:latest oleh-VirtualBox Running       Running 38 seconds ago                       
3e5993w0ynpb zen_morse.3 alpine:latest oleh-VirtualBox Running       Running 38 seconds ago 

If we force remove  one of the container via "docker container rm -f zen_morse.1.q410bnac..." 
swarm will automatically recover it.
To remove the whole swarm.
$>docker service rm zen_morse -> it will remove all containers (tasks) created in it also.

So for example we need to update something in container, without re-creating it, some env 
vars or something, or something with cp usage - we'll use docker update command. 
But what if our container in production and there a new version, and we don't want to turn
off the old one, until new one isn't started, because we don't want to lose any second of
our service being available? "docker service update" has some options for that.

Managers also do some work, since they are privileged workers.

To make a swarm:
On machine that you want to work with (as a manager)
$>docker swarm init --advertise-addr 192.186.0.1

Swarm initialized: current node (rzj79ruf66wkj38gs6ctjmdma) is now a manager.
To add a worker to this swarm, run the following command:
    docker swarm join --token SWMTKN-1-2d3bvhjb67r4irvqdvovhq3sran5tqy3sz8judccarsbt4c0j6-3 \
    yv5gyjvtdjo68eix4eftrmgt 192.168.0.13:2377
To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions

So you just copy and paste docker swarm join command to terminal of machine that you want
to add as a worker

manager_node$>docker node ls
ID                   HOSTNAME  STATUS AVAILABILITY MANAGER STATUS  ENGINE VERSION
rzj79ruf66wkj38g *   node1     Ready  Active       Leader          19.03.0-beta2
2u6bm1wlph425uvo     node2     Ready  Active                       19.03.0-beta2

* - is where you are now
As you can see there two nodes, and one is a leader.

worker_node$>docker node ls
Error response from daemon: This node is not a swarm manager. Worker nodes can't be used
to view or modify cluster state. Please run this command on a manager node or promote
the current node to a manager
We cannot use swarm commands in worker.

To promote worker to manager:
manager_machine$>docker node update --role manager 2u6bm1wlph425uvo (id of worker in swarm)

manager_node$>docker node ls
ID                   HOSTNAME  STATUS AVAILABILITY MANAGER STATUS  ENGINE VERSION
rzj79ruf66wkj38g *   node1     Ready  Active       Leader          19.03.0-beta2
2u6bm1wlph425uvo     node2     Ready  Active       Reachable       19.03.0-beta2

As you can see, new manager became reachable but leader is still leader.
To add manager by default
manager_machine$>docker swarm join-token manager

To add a manager to this swarm, run the following command:
docker swarm join --token SWMTKN-1-2d3bvhjb67r4irvqdvovhq3sran5tqy3sz8judccarsbt4c0j6-6 \
  7qr13gj7ksprmdkaef0zht91 192.168.0.13:2377

not_in_swarm_node$>docker swarm join --token SWMTKN-1-2d3bvhjb6...

We can change these keys for security reason.

And now let's do some work. And start it from leader node.
leader_node$>docker service create --replicas 3 alpine ping 8.8.8.8
leader_node$>docker service ls
ID                NAME                MODE                REPLICAS   IMAGE      
xpe0qb9al765      zen_shockley        replicated          3/3        alpine:latest
leader_node$>docker service ps zen_shockley
ID           NAME           IMAGE         NODE    DESIRED STATE   CURRENT STATE      
xkr8ou7sm7ea zen_shockley.1 alpine:latest node2   Running         Running 2 minutes ago
vi532301kl64 zen_shockley.2 alpine:latest node3   Running         Running 2 minutes ago
awt29rckai1b zen_shockley.3 alpine:latest node1   Running         Running 2 minutes ago

leader_node$>docker node ps
ID           NAME           IMAGE         NODE  DESIRED STATE  CURRENT STATE         
awt29rckai1b zen_shockley.3 alpine:latest node1 Running        Running 6 minutes ago

As you can see on leader machine 3rd service is running.
we can specify name of node to check what services it got
leader_node$>docker node ps node2 (node name, since it node command)

So as you notice - we can do most thing from leader machine, what is truly comfortable.

----------------------------------------------------------------------------------------------
Tricks with swarm
----------------------------------------------------------------------------------------------
# Swarm Basic Features and How to Use Them In Your Workflow
## Scaling Out with Overlay Networking
docker network create --driver overlay mydrupal

To create a named service and add it to specific network
docker service create --name psql --network mydrupal -e POSTGRES_PASSWORD=mypass postgres

docker service ps psql

to check logs in container.
docker container logs psql TAB COMPLETION (it will give you a big id of container in swarm)
database running on node1

create a drupal service
docker service create --name drupal --network mydrupal -p 80:80 drupal

To check in real time what's happening - we can add watch before ls command
watch docker service ls
drupal website running on node2

How do they talk to each other?
thru the service name.

Go to the ip:80 of drupal machine, and via UI config that host of database configuration
is psql (like service name). And it connects to it, this is possible because of overlay network
driver - it makes swarm acts as all nodes is in the same subnet. overlay - is a only one
network that we can use in the swarm to not have some problems with networking.
You can also see that if you'll go at any ip of your nodes running - all will be redirected
to drupal site, but drupal is running only on node2, how is it happening?
Because of routing mash.
Super cool load-balancer, that listens all traffic that come to all
nodes in swarm, and routes it to needed node. Since we using services and tasks - they can fail, 
re-create, re-run and so on, so they can change. And it could be a big pain in ass if you should
set ips, ports and firewalls for these containers each time ip is changed - mash do this job for you.
Basically it creates VIP for every service.
Bless docker.

to checkout service config.json file
docker service inspect drupal

##Create a Multi-Service Multi-Node Web App
docker node ls
docker service ls
docker network create -d overlay backend
docker network create -d overlay frontend
docker service create --name vote -p 80:80 --network frontend -- replica 2 COPY IMAGE
docker service create --name redis --network frontend --replica 1 redis:3.2
docker service create --name worker --network frontend --network backend COPY IMAGE
docker service create --name db --network backend COPY MOUNT INFO
docker service create --name result --network backend -p 5001:80 COPY INFO
docker service ls
docker service ps result
docker service ps redis
docker service ps db
docker service ps vote
docker service ps worker
cat /etc/docker/
docker service logs worker
docker service ps worker

## Swarm Stacks and Production Grade Compose
docker stack deploy -c example-voting-app-stack.yml voteapp
docker stack
docker stack ls
docker stack ps voteapp
docker container ls
docker stack services voteapp
docker stack ps voteapp
docker network ls
docker stack deploy -c example-voting-app-stack.yml voteapp

## Using Secrets in Swarm Services
docker secret create psql_usr psql_usr.txt
echo "myDBpassWORD" | docker secret create psql_pass - TAB COMPLETION
docker secret ls
docker secret inspect psql_usr
docker secret create --name psql --secret psql_user --secret psql_pass -e POSTGRES_PASSWORD_FILE=/run/secrets/psql_pass -e POSTGRES_USER_FILE=/run/secrets/psql_user postgres
docker service ps psql
docker exec -it psql.1.CONTAINER NAME bash
docker logs TAB COMPLETION
docker service ps psql
docker service update --secret-rm

## Using Secrets with Swarm Stacks
vim docker-compose.yml
docker stack deploy -c docker-compose.yml mydb
docker secret ls
docker stack rm mydb

##Create A Stack with Secrets and Deploy
vim docker-compose.yml
docker stack deploy - c docker-compose.yml drupal
echo STRING |docker secret create psql-ps - VALUE
docker stack deploy -c docker-compose.yml drupal
docker stack ps drupal